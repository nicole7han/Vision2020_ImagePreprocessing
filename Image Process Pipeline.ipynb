{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image Process Pipeline.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"M4jixcFjRnA3","colab_type":"code","outputId":"3ecf179d-eefe-4836-ce1e-43f7097fba65","executionInfo":{"status":"ok","timestamp":1590586275884,"user_tz":420,"elapsed":33996,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# Mounting drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DHmI8qflpqp-","colab_type":"text"},"source":["# mask RCNN"]},{"cell_type":"code","metadata":{"id":"XWOn9oJK-ABu","colab_type":"code","outputId":"5cdc9151-053b-49df-e92d-56a8ebe9c248","executionInfo":{"status":"ok","timestamp":1590586280068,"user_tz":420,"elapsed":17189,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# !pip install tensorflow==1.13.1\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jAEsEpzkgxpq","colab_type":"code","outputId":"c267730a-d4a9-4978-9f12-e6f81b370564","executionInfo":{"status":"ok","timestamp":1590586282562,"user_tz":420,"elapsed":18989,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# !git clone https://github.com/matterport/Mask_RCNN.git\n","import os\n","os.chdir('/content/gdrive/My Drive/Mask_RCNN/samples/coco')\n","!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["coco.py\t\t    inspect_model.ipynb    __pycache__\n","inspect_data.ipynb  inspect_weights.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UCddmroZSmHp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f28e94a8-6e2d-4b7e-9005-dc854678897a","executionInfo":{"status":"ok","timestamp":1590586286936,"user_tz":420,"elapsed":20311,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}}},"source":["import os\n","import glob\n","import sys\n","import random\n","import math\n","import re\n","import time\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.axes as ax\n","import json\n","import skimage.io\n","from skimage import measure\n","from scipy.io import loadmat\n","from scipy.spatial import distance\n","import cv2\n","from PIL import Image\n","\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(\"../../\")\n","\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR)  # To find local version of the library\n","from mrcnn import utils\n","from mrcnn import visualize\n","from mrcnn.visualize import display_images\n","import mrcnn.model as modellib\n","from mrcnn.model import log\n","from mrcnn.utils import Dataset\n","\n","%matplotlib inline \n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n","\n","# Local path to trained weights file\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","# Download COCO trained weights from Releases if needed\n","if not os.path.exists(COCO_MODEL_PATH):\n","    utils.download_trained_weights(COCO_MODEL_PATH)\n","\n","# Path to Shapes trained weights\n","SHAPES_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_shapes.h5\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zzb2w02eSr72","colab_type":"code","colab":{}},"source":["import coco\n","config = coco.CocoConfig()\n","COCO_DIR = \"/content/coco\" \n","\n","# Override the training configurations with a few\n","# changes for inferencing.\n","class InferenceConfig(config.__class__):\n","    # Run detection on one image at a time\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","config = InferenceConfig()\n","# config.display()\n","\n","\n","# Device to load the neural network on.\n","# Useful if you're training a model on the same \n","# machine, in which case use CPU and leave the\n","# GPU for training.\n","DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n","\n","# Inspect the model in training or inference modes\n","# values: 'inference' or 'training'\n","# TODO: code for 'training' test mode not ready yet\n","TEST_MODE = \"inference\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VVgbMyBSx_7","colab_type":"code","colab":{}},"source":["def get_ax(rows=1, cols=1, size=16):\n","    \"\"\"Return a Matplotlib Axes array to be used in\n","    all visualizations in the notebook. Provide a\n","    central point to control graph sizes.\n","    \n","    Adjust the size attribute to control how big to render images\n","    \"\"\"\n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KFbPKaCS0uK","colab_type":"code","outputId":"c02d61e4-c2fd-4c76-da63-9b643d67c87f","executionInfo":{"status":"ok","timestamp":1590586313058,"user_tz":420,"elapsed":38188,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["# Create model in inference mode\n","with tf.device(DEVICE):\n","    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n","                              config=config)\n","\n","# Set weights file path\n","if config.NAME == \"shapes\":\n","    weights_path = SHAPES_MODEL_PATH\n","elif config.NAME == \"coco\":\n","    weights_path = COCO_MODEL_PATH\n","# Or, uncomment to load the last model you trained\n","# weights_path = model.find_last()\n","\n","# Load weights\n","print(\"Loading weights \", weights_path)\n","model.load_weights(weights_path, by_name=True)\n","\n","class_names =  ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Mask_RCNN/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Mask_RCNN/mrcnn/model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/gdrive/My Drive/Mask_RCNN/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /content/gdrive/My Drive/Mask_RCNN/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Mask_RCNN/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","Loading weights  /content/gdrive/My Drive/Mask_RCNN/mask_rcnn_coco.h5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5TBYKKjrpyUQ","colab_type":"text"},"source":["# Deep Gaze II"]},{"cell_type":"code","metadata":{"id":"bfZ4nZhrp0XF","colab_type":"code","outputId":"724ae8ad-1cfa-4a87-9c84-93ae5255b419","executionInfo":{"status":"ok","timestamp":1590586315440,"user_tz":420,"elapsed":29807,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","os.chdir('/content/gdrive/My Drive')\n","!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["'1st figure in the video.fig'\n","'291 Bionic Vision Project'\n","'a1 (1).m'\n"," a1.m\n","'Cog Neuro Final_Sudhanshu Srivastava.gdoc'\n","'Colab Notebooks'\n","'Cont Cuing.gdoc'\n","'Contextual Cueing IO.gdoc'\n","'contrast_ttest (1).m'\n"," contrast_ttest.m\n"," CS291\n","'CS291I-C1-Introduction (1).pptx'\n"," CS291I-C2-Sight-Recovery-Technologies.pptx\n"," deepgaze\n","'diff_minus_slope (1).m'\n"," diff_minus_slope.m\n","'DNNs learn to attend.gslides'\n","'draw_box (1).m'\n"," draw_box.m\n","'draw_slope (1).m'\n","'draw_slope2 (1).m'\n"," draw_slope2.m\n","'draw_slope_func (1).m'\n"," draw_slope_func.m\n"," draw_slope.m\n"," Experiments:.gdoc\n","'first level results.fig'\n","'fMRI analysis Quiz 3 keywords.gdoc'\n","'generate_images (1).m'\n"," generate_images.m\n","'gen_images (1).m'\n"," gen_images.m\n"," gen_test_images.m\n","'Getting started.pdf'\n","'HW 3_Sudhanshu.gdoc'\n","'HW 4.gdoc'\n"," HW6_Sudhanshu.gdoc\n"," imagecreate.m\n","'ImageNet Trained CNNs are biased towards texture.gslides'\n"," imdiltest.m\n"," IMG_0753.mov\n"," IMG_20190327_154437531_HDR.jpg\n"," IMG_20190327_154446885_HDR.jpg\n"," IMG_20190327_154454194.jpg\n"," IMG_20190327_160238467_HDR.jpg\n"," IMG_20190327_160240419.jpg\n"," IMG_20190327_160243314.jpg\n"," IMG_20200219_170339437.jpg\n","'Introduction to Convolutional Neural Networks.gslides'\n"," jitterboth.m\n"," jitter.m\n"," level2.m\n","'Lin Systems.gdoc'\n"," Mask_RCNN\n","'model1 response and pred.jpg'\n","'model results.gdoc'\n"," models\n"," myfile.txt\n"," nas_backup\n"," NB_SoP_v1.docx\n"," NB_SoP_v1.gdoc\n"," noisepluscontrast.fig\n"," only_box.m\n"," only_lines.m\n","'optical flow'\n"," Perceptron.gslides\n","'Posner and Cue Combination IO and CNN.gdoc'\n","'Posner Visualization.gdoc'\n","'Psy 231 Week 3 Write-up.gdoc'\n"," Questions.gdoc\n","'Reading List superset.gdoc'\n"," Results\n"," runtf2.ipynb\n"," runtfbetter.ipynb\n"," runtf.ipynb\n","'Search>Track: everything involved in template comparison, working memory, attention should get represented.gdoc'\n"," small_test.m\n"," small_train.m\n"," Solvang\n","'Solvang 2019'\n","'Solvang - Spring break 2019'\n"," Sudhanshu\n"," Supplementary_Material_Sudhanshu.gslides\n","'The Brain Network Underlying Serial Visual Search: Comparing Overt and Covert Spatial Orienting, for Activations and for Effective Connectivity.gslides'\n","'timeseries with fwe.fig'\n","'timeseries without fwe.fig'\n"," Untitled1.ipynb\n","'Untitled document.gdoc'\n"," Untitled.ipynb\n"," v1_timeseries.jpg\n","'Visualization Techniques for CNNs.gslides'\n"," VS_config2_n1.zip\n","'VSS Abstract.gdoc'\n","'Week 4 Journal Club write-up       Sudhanshu Srivastava.gdoc'\n","'Week 5 Journal Club write-up (1).gdoc'\n","'Week 5 Journal Club write-up.gdoc'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K8UzrDaEp3wr","colab_type":"code","colab":{}},"source":["# !git clone https://github.com/mpatacchiola/deepgaze.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XuZy49pDqHGt","colab_type":"code","outputId":"f4812aae-f2ed-4837-c1d6-cd6c10b79ce2","executionInfo":{"status":"ok","timestamp":1590586317457,"user_tz":420,"elapsed":16494,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["os.chdir(\"/content/gdrive/My Drive/deepgaze\")\n","!ls"],"execution_count":9,"outputs":[{"output_type":"stream","text":["debug.log  deepgaze  doc  etc  examples  LICENSE  README.md  setup.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sa5BEdm7p-WJ","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python\n","\n","# The MIT License (MIT)\n","# Copyright (c) 2017 Massimiliano Patacchiola\n","# https://mpatacchiola.github.io\n","# https://mpatacchiola.github.io/blog/\n","#\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n","# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n","# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n","# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","\n","# In this example the FASA algorithm is used in order to process some images.\n","# The original image and the saliency version are showed for comparison.\n","\n","import numpy as np\n","import cv2\n","from timeit import default_timer as timer\n","from deepgaze.saliency_map import FasaSaliencyMapping \n","import os\n","import glob\n","import sys\n","import random\n","import math\n","import re\n","import time\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.axes as ax\n","import json\n","import skimage.io\n","from scipy.io import loadmat\n","import cv2\n","from PIL import Image\n","\n","# def main():\n","\n","#     image_1 = cv2.imread(\"./horse.jpg\")\n","#     image_2 = cv2.imread(\"./car.jpg\")\n","#     image_3 = cv2.imread(\"./plane.jpg\")\n","#     image_4 = cv2.imread(\"./pear.jpg\")\n","\n","#     # for each image the same operations are repeated\n","#     my_map = FasaSaliencyMapping(image_1.shape[0], image_1.shape[1])  # init the saliency object\n","#     start = timer()\n","#     image_salient_1 = my_map.returnMask(image_1, tot_bins=8, format='BGR2LAB')  # get the mask from the original image\n","#     image_salient_1 = cv2.GaussianBlur(image_salient_1, (3,3), 1)  # applying gaussin blur to make it pretty\n","#     end = timer()\n","#     print(\"--- %s Image 1 tot seconds ---\" % (end - start))\n","\n","#     my_map = FasaSaliencyMapping(image_2.shape[0], image_2.shape[1])\n","#     start = timer()\n","#     image_salient_2 = my_map.returnMask(image_2, tot_bins=8, format='BGR2LAB')\n","#     image_salient_2 = cv2.GaussianBlur(image_salient_2, (3,3), 1)\n","#     end = timer()\n","#     print(\"--- %s Image 2 tot seconds ---\" % (end - start))\n","\n","#     my_map = FasaSaliencyMapping(image_3.shape[0], image_3.shape[1])\n","#     start = timer()\n","#     image_salient_3 = my_map.returnMask(image_3, tot_bins=8, format='BGR2LAB')\n","#     #image_salient_3 = cv2.GaussianBlur(image_salient_3, (3,3), 1)\n","#     end = timer()\n","#     print(\"--- %s Image 3 tot seconds ---\" % (end - start))\n","\n","#     my_map = FasaSaliencyMapping(image_4.shape[0], image_4.shape[1])\n","#     start = timer()\n","#     image_salient_4 = my_map.returnMask(image_4, tot_bins=8, format='BGR2LAB')\n","#     image_salient_4 = cv2.GaussianBlur(image_salient_4, (3,3), 1)\n","#     end = timer()\n","#     print(\"--- %s Image 4 tot seconds ---\" % (end - start))\n","\n","#     # Creating stack of images and showing them on screen\n","#     original_images_stack = np.hstack((image_1, image_2, image_3, image_4))\n","#     saliency_images_stack = np.hstack((image_salient_1, image_salient_2, image_salient_3, image_salient_4))\n","#     saliency_images_stack = np.dstack((saliency_images_stack,saliency_images_stack,saliency_images_stack))\n","#     cv2.imshow(\"Original-Saliency\", np.vstack((original_images_stack, saliency_images_stack)))\n","\n","#     while True:\n","#         if cv2.waitKey(33) == ord('q'):\n","#             cv2.destroyAllWindows()\n","#             break\n","\n","\n","# if __name__ == \"__main__\":\n","#     main()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IwsbTIw6efTJ","colab_type":"text"},"source":["# Segmentation * (Saliency + Optical flow)\n"]},{"cell_type":"code","metadata":{"id":"pMq2WSdemMOA","colab_type":"code","colab":{}},"source":["from skimage import feature\n","from sklearn.preprocessing import normalize\n","import glob"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RysFivFbfD8G","colab_type":"text"},"source":["### functions"]},{"cell_type":"code","metadata":{"id":"8y0aF9OXfDgs","colab_type":"code","colab":{}},"source":["def sal_mask(image):\n","  '''\n","  PARAMETERS:\n","    image: input image [h, w, 3]\n","  \n","  RETURN:\n","    origs: original images [h, w, 3, #images]\n","    origs_gray: original images in grayscale [h, w, #images]\n","    saliens: deepgaze saliency map of images [h, w, #images]\n","    masks: combined object masks of images [h, w, #images]\n","  '''\n","  h,w,_ = image.shape\n","\n","  # get masks\n","  results = model.detect([image])\n","  r = results[0]\n","  obj_masks = r[\"masks\"]\n","  # fig = visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n","  #                           class_names, r['scores'])\n","\n","  # get mask edges\n","  num_objs = r[\"masks\"].shape[2]\n","  obj_edges = np.zeros(r[\"masks\"].shape)\n","  for o in range(num_objs):\n","    m = r[\"masks\"][:,:,o]\n","    edges = feature.canny(m, sigma=5).astype(\"uint8\")\n","    kernel = np.ones((10,10), np.uint8) \n","    edges_dilated = cv2.dilate(edges,kernel)\n","    obj_edges[:,:,o] = edges_dilated\n","\n","  # # get all object masks into one mask\n","  # combined_masks = np.sum(r['masks'],2)\n","\n","  # get saliency map\n","  my_map = FasaSaliencyMapping(h, w) \n","  image_salient = my_map.returnMask(image, tot_bins=8, format='BGR2LAB')  # get the mask from the original image\n","  image_salient_blr = cv2.GaussianBlur(image_salient, (3,3), 1)\n","\n","  return image_salient_blr, obj_masks, obj_edges\n","\n","\n","def intersec_area(a, b):  # returns 0 if rectangles don't intersect\n","    dx = np.min([a[3], b[3]]) - np.max([a[1], b[1]])\n","    dy = np.mean([a[2], b[2]]) - np.max([a[0], b[0]])\n","    if (dx>=0) and (dy>=0):\n","        return dx*dy\n","    else: \n","      return 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvWODZTwfJfy","colab_type":"text"},"source":["### read original video and set up"]},{"cell_type":"code","metadata":{"id":"BWWRslkigCIc","colab_type":"code","colab":{}},"source":["vid_path = \"/content/gdrive/My Drive/291 Bionic Vision Project/Asa_Video/stim18_vid\" # ORIGINAL VIDEO PATH FOLDER \n","vid_name = 'stim18.mp4'\n","\n","cap = cv2.VideoCapture('%s/%s'% (vid_path, vid_name))\n","ret, frame1 = cap.read()\n","\n","results = model.detect([frame1])\n","rs = results[0]\n","# fig = visualize.display_instances(frame1, rs['rois'], rs['masks'], rs['class_ids'], \n","                          # class_names, rs['scores'])\n","\n","\n","# set up optical flow \n","prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n","hsv = np.zeros_like(frame1)\n","hsv[...,1] = 255\n","\n","# saliency observation window\n","W = 5\n","w_counter = 1\n","s, m, e = sal_mask(frame1)\n","s_history = np.copy(s)\n","s_history = np.expand_dims(s_history, axis=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_AzZuTy5nZq","colab_type":"text"},"source":["### Segmentation (enhanced by optical flow) + Saliency (smoothied from previous frames and filted out large variance)\n"]},{"cell_type":"code","metadata":{"id":"SYeHj-oovxw7","colab_type":"code","outputId":"6663facc-edce-4464-cf38-938d5d9628c7","executionInfo":{"status":"ok","timestamp":1590602696104,"user_tz":420,"elapsed":1288277,"user":{"displayName":"Sudhanshu Srivastava","photoUrl":"","userId":"15790548182479666216"}},"colab":{"base_uri":"https://localhost:8080/","height":248}},"source":["### create processed frames and downsampled frames ###\n","k=1\n","while(1):\n","    ret, frame2 = cap.read()\n","    if ret == True:\n","      next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n","\n","      # get saliency (h,w) and masks, edges (h,w,#objects), saliency histories\n","      s, m, e = sal_mask(frame1)\n","      s2, m2, e2 = sal_mask(frame2)\n","      s_history = np.concatenate([s_history, np.expand_dims(s2, axis=2)], axis = 2)\n","\n","      if s_history.shape[2] < W: # if not observed 5 frames yet\n","        s_orig = np.copy(s)\n","      else: # if observed 5 frames\n","        recent_saliens = s_history[:,:,-W:]\n","        recent_saliens_var = np.var(recent_saliens, axis = 2) #calculate recent saliency variance\n","        recent_saliens_var = normalize(recent_saliens_var)\n","        recent_saliens_var[recent_saliens_var>5e-05] = 0\n","        s_orig = np.mean(recent_saliens, axis = 2) #take mean of saliency maps\n","        np.putmask(s_orig, recent_saliens_var==0, 0)\n","        \n","      e_combined = np.sum(e, axis = 2)\n","      m_combined = np.sum(m, axis = 2)\n","      num_objs = m.shape[2]\n","\n","      # optical flow\n","      flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n","      mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n","      hsv[...,0] = ang*180/np.pi\n","      hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n","      \n","      if num_objs > 0:\n","        # compute optical flow and saliency for each object within the mask\n","        o_opt = []\n","        o_s = []\n","        for o in range(num_objs):\n","          o_m = m[:,:,o]\n","          o_mag = np.mean(hsv[:,:,2] * o_m)\n","          o_opt.append(o_mag)\n","          o_s.append((s*o_m).mean())\n","\n","        o_opt_n = normalize(np.array(o_opt).reshape(1,-1))\n","        o_s_n = normalize(np.array(o_s).reshape(1,-1))\n","\n","        #highlight mask of e with high optical flow magnitude\n","        #highlight saliency of object mask \n","        for o in range(num_objs):\n","          o_e = e[:,:,o]\n","          o_m = m[:,:,o]\n","          np.putmask(m_combined, o_m, 255 * o_opt_n[0][o])\n","\n","      out = np.maximum(s_orig, m_combined)\n","      # out /= out.max()/1.0 \n","      out_d = cv2.resize(out.astype('float64'), (21,21), interpolation=cv2.INTER_LANCZOS4)\n","\n","      plt.imshow(out,'gray')\n","      plt.axis('off')\n","      plt.savefig(\"%s/frame_%d.jpg\" % (vid_path,k), bbox_inches='tight', pad_inches=0)\n","\n","      plt.imshow(out_d, \"gray\")\n","      plt.axis(\"off\")\n","      plt.savefig(\"%s/frame_%d_d.jpg\" % (vid_path,k), bbox_inches='tight', pad_inches=0)\n","\n","      k += 1\n","      prv = next\n","      frame1 = frame2\n","\n","    else:\n","      break\n","\n","cv2.destroyAllWindows()\n","cap.release()"],"execution_count":26,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"EOTXgzru9NSK","colab_type":"code","colab":{}},"source":["cv2.destroyAllWindows()\n","cap.release()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Z84uK8h5ZKR","colab_type":"text"},"source":["## create final video"]},{"cell_type":"code","metadata":{"id":"_k665s9V5ag_","colab_type":"code","colab":{}},"source":["import cv2\n","import numpy as np\n","import glob\n","from os.path import isfile, join\n","pathIn = vid_path\n","pathOut = '%s/clip.avi' % pathIn\n","frame_array = []\n","files = glob.glob('%s/*d.jpg'%pathIn) # read final downsampled frames\n","fps = 28\n","\n","img = cv2.imread(files[0])\n","height, width, layers = img.shape\n","size = (width,height)\n","\n","out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n","for i in range(len(files)):\n","    filename= files[i]\n","    img = cv2.imread(filename)\n","    out.write(img)\n","\n","out.release()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3vpHJK1DYbE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
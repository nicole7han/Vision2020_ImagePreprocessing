{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"Segmentation.ipynb","provenance":[{"file_id":"https://github.com/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb","timestamp":1601437280022}],"collapsed_sections":["KKR-4AfLNvYW","zh2rcVIiNw7L","GbHMLG2tNe9O","NIWJssKaKZQN"],"toc_visible":true,"machine_shape":"hm"}},"cells":[{"cell_type":"code","metadata":{"id":"F9ALCqvqAWE0","executionInfo":{"status":"ok","timestamp":1601996325055,"user_tz":420,"elapsed":18837,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}},"outputId":"a376616b-eb17-4a87-a6de-240de750e873","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Mounting drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"07TGccVeFck3"},"source":["### Environment Setup\n","\n","First, download the code and pretrained models if we are on colab."]},{"cell_type":"code","metadata":{"id":"_VsLTHwEzdfJ","executionInfo":{"status":"ok","timestamp":1601996327202,"user_tz":420,"elapsed":20976,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}},"outputId":"70b288a4-7e19-4321-bf63-baac866836fd","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["import glob, os, cv2, shutil\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image, ImageFilter \n","from skimage import morphology\n","os.chdir(\"/content/gdrive/My Drive/segmentation\")\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["config\t\t      encoder_epoch_20.pth  mit_semseg\t      teaser\n","data\t\t      eval_multipro.py\t    notebooks\t      test.py\n","decoder_epoch_20.pth  eval.py\t\t    README.md\t      train.py\n","demo_test.sh\t      install.log\t    requirements.txt\n","download_ADE20K.sh    LICENSE\t\t    setup.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WQv6JnDSFck4","executionInfo":{"status":"ok","timestamp":1601996327203,"user_tz":420,"elapsed":20975,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}}},"source":["# %%bash\n","# # Colab-specific setup\n","# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n","# pip install yacs 2>&1 >> install.log\n","# git init 2>&1 >> install.log\n","# git remote add origin https://github.com/CSAILVision/semantic-segmentation-pytorch.git 2>> install.log\n","# git pull origin master 2>&1 >> install.log\n","# DOWNLOAD_ONLY=1 ./demo_test.sh 2>> install.log"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9RYbVAhFck8","executionInfo":{"status":"ok","timestamp":1601996367295,"user_tz":420,"elapsed":20681,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}}},"source":["# System libs\n","import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n","# Our libs\n","from mit_semseg.models import ModelBuilder, SegmentationModule\n","from mit_semseg.utils import colorEncode\n","\n","colors = scipy.io.loadmat('data/color150.mat')['colors']\n","names = {}\n","with open('data/object150_info.csv') as f:\n","    reader = csv.reader(f)\n","    next(reader)\n","    for row in reader:\n","        names[int(row[0])] = row[5].split(\";\")[0]\n","\n","def visualize_result(img, pred, index=None):\n","    # filter prediction class if requested\n","    if index is not None:\n","        pred = pred.copy()\n","        pred[pred != index] = -1\n","        print(f'{names[index+1]}:')\n","        \n","    # colorize prediction\n","    pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n","\n","    # aggregate images and save\n","    im_vis = numpy.concatenate((img, pred_color), axis=1)\n","    display(PIL.Image.fromarray(im_vis))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ETpVQSoGFck_"},"source":["## Loading the segmentation model"]},{"cell_type":"code","metadata":{"id":"X-UIhsfwFck_","executionInfo":{"status":"ok","timestamp":1601996385976,"user_tz":420,"elapsed":38564,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}},"outputId":"ff2fbeca-b981-49e7-94b9-585c412924c0","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Network Builders\n","net_encoder = ModelBuilder.build_encoder(\n","    arch='resnet50dilated',\n","    fc_dim=2048,\n","    weights='encoder_epoch_20.pth')\n","net_decoder = ModelBuilder.build_decoder(\n","    arch='ppm_deepsup',\n","    fc_dim=2048,\n","    num_class=150,\n","    weights='decoder_epoch_20.pth',\n","    use_softmax=True)\n","\n","crit = torch.nn.NLLLoss(ignore_index=-1)\n","segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n","segmentation_module.eval()\n","segmentation_module.cuda()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Loading weights for net_encoder\n","Loading weights for net_decoder\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["SegmentationModule(\n","  (encoder): ResnetDilated(\n","    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","    (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","    (relu1): ReLU(inplace=True)\n","    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","    (relu2): ReLU(inplace=True)\n","    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (bn3): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","    (relu3): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n","        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (decoder): PPMDeepsup(\n","    (ppm): ModuleList(\n","      (0): Sequential(\n","        (0): AdaptiveAvgPool2d(output_size=1)\n","        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (3): ReLU(inplace=True)\n","      )\n","      (1): Sequential(\n","        (0): AdaptiveAvgPool2d(output_size=2)\n","        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (3): ReLU(inplace=True)\n","      )\n","      (2): Sequential(\n","        (0): AdaptiveAvgPool2d(output_size=3)\n","        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (3): ReLU(inplace=True)\n","      )\n","      (3): Sequential(\n","        (0): AdaptiveAvgPool2d(output_size=6)\n","        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","        (3): ReLU(inplace=True)\n","      )\n","    )\n","    (cbr_deepsup): Sequential(\n","      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (conv_last): Sequential(\n","      (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Dropout2d(p=0.1, inplace=False)\n","      (4): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (conv_last_deepsup): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))\n","    (dropout_deepsup): Dropout2d(p=0.1, inplace=False)\n","  )\n","  (crit): NLLLoss()\n",")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"TWqfXmDkFclB"},"source":["## Load test data\n","\n","Now we load and normalize a single test image.  Here we use the commonplace convention of normalizing the image to a scale for which the RGB values of a large photo dataset would have zero mean and unit standard deviation.  (These numbers come from the imagenet dataset.)  With this normalization, the limiiting ranges of RGB values are within about (-2.2 to +2.7)."]},{"cell_type":"code","metadata":{"id":"2lh-4yPnFclC","executionInfo":{"status":"ok","timestamp":1601996385980,"user_tz":420,"elapsed":37197,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}}},"source":["# Load and normalize one image as a singleton tensor batch\n","pil_to_tensor = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n","        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n","])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"944EFQdP_N-T"},"source":["## version4: people, car, bus, truck mask + path edges (smooth)"]},{"cell_type":"code","metadata":{"id":"50gJzAwQqp24","executionInfo":{"status":"ok","timestamp":1601996459143,"user_tz":420,"elapsed":1186,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}}},"source":["def get_houghlines(edges):\n","  kernel = np.ones((10,10), np.uint8)\n","  edge_history = cv2.HoughLinesP(edges.astype(\"uint8\"),1,np.pi/180,15,minLineLength=minLineLength,maxLineGap=maxLineGap)\n","  edge_combined = np.zeros(edges.shape)\n","  try:\n","    for x in range(0, len(edge_history)):\n","      for x1,y1,x2,y2 in edge_history[x]:\n","        if np.abs(x1-x2)>5 and np.abs(y1-y2)>5:# we don't want edges in the border\n","          cv2.line(edge_combined,(x1,y1),(x2,y2),color = (255, 255, 255))\n","    edge_combined = cv2.dilate(edge_combined, kernel, iterations=1)\n","  except (RuntimeError, TypeError, NameError):\n","    print(\"no lines\")\n","  return edge_combined"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"fq5Ayv4hE6G-","executionInfo":{"status":"ok","timestamp":1601996484099,"user_tz":420,"elapsed":1099,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}},"outputId":"0f18b3cb-9010-4e12-c138-3c411ed4df51","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["vid_path = \"/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos\"\n","all_folders = glob.glob(\"%s/*_frames\" %vid_path)\n","all_folders.sort()\n","all_folders[5:8]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos/stim15_orig_frames',\n"," '/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos/stim16_orig_frames',\n"," '/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos/stim1_orig_frames']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"91nFtuMC_TXm","executionInfo":{"status":"error","timestamp":1601996493775,"user_tz":420,"elapsed":4855,"user":{"displayName":"VIU Lab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWt9J9AZkxA22Vv1yk6rjSzZH6UU5QbP2HNaK7=s64","userId":"11080165632121837049"}},"outputId":"acbc9746-3caf-47f8-d41b-ba8c80859d30","colab":{"base_uri":"https://localhost:8080/","height":628}},"source":["vid_path = \"/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos\"\n","all_folders = glob.glob(\"%s/*_frames\" %vid_path)\n","all_folders.sort()\n","all_folders = all_folders[5:8]\n","for f in all_folders: #each video\n","  stim_name = f.split(\"/\")[-1].split(\"_\")[0]\n","  print(stim_name)\n","  all_frames = glob.glob(\"%s/*.jpg\" %f)\n","  \n","  W = 10\n","  w_count = 0\n","  mask_rep = np.zeros((4,540, 960, W)) #num_class x hieght x width x windowsize\n","  #classes in order [12,20,80,83] people, car, bus, truck\n","  edge_rep = np.zeros((540, 960, W))\n","\n","  for count in np.arange(1,len(all_frames)): #each frame\n","    f_name = f + \"/frame%d.jpg\" %count\n","    pil_image = PIL.Image.open(f_name).convert('RGB')\n","    img_original = numpy.array(pil_image)\n","    img_data = pil_to_tensor(pil_image)\n","    singleton_batch = {'img_data': img_data[None].cuda()}\n","    output_size = img_data.shape[1:]\n","\n","    with torch.no_grad():\n","      scores = segmentation_module(singleton_batch, segSize=output_size)\n","\n","    # Get the predicted scores for each pixel\n","    _, pred = torch.max(scores, dim=1)\n","    pred = pred.cpu()[0].numpy()\n","    # visualize_result(img_original, pred)\n","\n","    # filter out other classes\n","    classes = [6, 11, 12, 13, 17, 20] #road, sidewalk, person, earth, plant, car\n","    pred_clean = pred.copy()\n","    pred_clean[~np.isin(pred_clean, classes)]= 0\n","\n","    # filter out small islands\n","    pred_clean2 = morphology.remove_small_objects(pred_clean.astype(bool), min_size=16000).astype(int)*255\n","\n","    # combine mask with correct class labels\n","    pred_clean3 = np.minimum(pred_clean, pred_clean2)\n","\n","    # get structure edges and get only long ones\n","    image = Image.fromarray(np.uint8(pred_clean3 * 255) , 'L')\n","    image_edge = image.filter(ImageFilter.FIND_EDGES) \n","    image_edge = np.array(image_edge)\n","    kernel = np.ones((10,10), np.uint8)\n","    image_edge = cv2.dilate(image_edge, kernel, iterations=1)\n","\n","    minLineLength = 200\n","    maxLineGap = 2\n","    lines = cv2.HoughLinesP(image_edge,1,np.pi/180,15,minLineLength=minLineLength,maxLineGap=maxLineGap)\n","    edges = np.zeros(pred_clean3.shape)\n","    try:\n","      for x in range(0, len(lines)):\n","        for x1,y1,x2,y2 in lines[x]:\n","          if x1!=x2 and y1!=y2:# we don't want edges in the border\n","            cv2.line(edges,(x1,y1),(x2,y2),color = (255, 255, 255))\n","      edges = cv2.dilate(edges, kernel, iterations=1)\n","    except (RuntimeError, TypeError, NameError):\n","      print(\"no lines\")\n","    \n","    ##############\n","    if count <= W: \n","      edge_rep[:,:,count-1] = edges\n","    else:\n","      #update current edge\n","      hist_curr = np.concatenate([edge_rep, np.expand_dims(edges,2)], axis = 2)\n","      hist_curr = np.max(hist_curr, axis = 2)\n","      plt.imshow(hist_curr)\n","      hist_curr = cv2.erode(get_houghlines(hist_curr), np.ones((10,10)))\n","      plt.imshow(hist_curr)\n","      kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n","      (thresh, binRed) = cv2.threshold(hist_curr, 0, 255, cv2.THRESH_BINARY)\n","      hist_curr = cv2.morphologyEx(hist_curr, cv2.MORPH_OPEN, kernel2, iterations=3)\n","      hist_curr = cv2.erode(get_houghlines(hist_curr), np.ones((10,10)))\n","      edges = hist_curr\n","      plt.imshow(edges)\n","    ##############\n","\n","    # update pred\n","    pred = pred_clean3\n","    allcounts = numpy.bincount(pred.flatten())\n","    predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]\n","    predicted_classes = [c for c in predicted_classes if c in [12, 20, 80, 83]] #mask only for person car, bus, truck\n","    classes_order = [12,20,80,83]\n","    mask_count = 1\n","    final_mask = np.zeros(pred.shape)\n","    for c in predicted_classes:\n","      class_name = names[c+1]\n","      mask_c = pred.copy()\n","      mask_c[mask_c == c] = 255\n","      mask_c[mask_c != 255] = 0\n","      mask_c = mask_c.astype(np.uint8)\n","      mask_size = len(np.where(mask_c==255)[0])\n","      \n","      if c==12 and mask_size>0: # find elongated direction for person\n","        # find outer contour\n","        ret, thresh = cv2.threshold(mask_c,0,255,0)\n","        cntrs = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","        cntrs = cntrs[0] if len(cntrs) == 2 else cntrs[1]\n","\n","        # get rotated rectangle from outer contour\n","        rotrect = cv2.minAreaRect(cntrs[0])\n","        box = cv2.boxPoints(rotrect)\n","        box = np.int0(box)\n","        # get angle from rotated rectangle\n","        angle = rotrect[-1]\n","        if np.abs(angle) < 60: #if the person elongated direction is horizontal, skip it (probably false alarm)\n","          mask_c = np.zeros(pred.shape)\n","\n","      #############\n","      # update mask and edge repo\n","      # if not seen 10 frames yet (count starts from 1)\n","      if count <= W: \n","        c_idx = classes_order.index(c)\n","        mask_rep[c_idx, :,:,count-1] = mask_c\n","      else: \n","        c_idx = classes_order.index(c)\n","        #1.find masks existed in the past\n","        mask_max = np.max(mask_rep[0, :,:,:], axis = 2)\n","        if mask_size == 0: # if the newest frame does not detect class\n","          mask_c = mask_max.copy()\n","        #2.update mask repository\n","        mask_old  = mask_rep[c_idx, :,:,:]\n","        mask_new = np.concatenate([mask_old, np.expand_dims(mask_c, axis = 2)], axis = 2)\n","        mask_new = np.delete(mask_new, 0, 2)\n","        mask_rep[c_idx,:,:,:] = mask_new\n","      #############\n","\n","\n","      # print(\"{} size {}\".format(class_name, mask_size))\n","      if mask_count == 1:\n","        final_mask = mask_c\n","        final_mask = np.expand_dims(final_mask, axis=2)\n","        mask_count +=1\n","      else:\n","        final_mask = np.concatenate([final_mask, np.expand_dims(mask_c, axis=2)], axis = 2)\n","        mask_count +=1\n","\n","    if len(final_mask.shape)==3:\n","      final_mask = np.concatenate([final_mask, np.expand_dims(edges, axis=2)], axis = 2)\n","      final_mask = np.max(final_mask, axis = 2)\n","    else:\n","      final_mask = np.concatenate([np.expand_dims(final_mask,axis=2), np.expand_dims(edges, axis=2)], axis = 2)\n","      final_mask = np.max(final_mask, axis = 2)\n","\n","    if os.path.isdir(\"%s/temp_mask_edge\" %(f))== False:\n","      os.mkdir(\"%s/temp_mask_edge\" %(f))\n","\n","    print(\"frame %d\" %count)\n","    plt.imshow(final_mask, \"gray\")\n","    plt.axis(\"off\")\n","    filename = \"frame_%d_seg.jpg\" %count\n","    plt.savefig(\"%s/temp_mask_edge/%s\" %(f,filename), bbox_inches='tight', pad_inches=0)\n","\n","    count +=1\n","\n","  print(\"%s finished\" %stim_name)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["stim15\n","frame 1\n"],"name":"stdout"},{"output_type":"error","ename":"PermissionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-75943a696c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"frame_%d_seg.jpg\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/temp_mask_edge/%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_jpg\u001b[0;34m(self, filename_or_obj, dryrun, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             return background.save(\n\u001b[0;32m--> 600\u001b[0;31m                 filename_or_obj, format='jpeg', **pil_kwargs)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mprint_jpeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_jpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2097\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos/stim15_orig_frames/temp_mask_edge/frame_1_seg.jpg'"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"KKR-4AfLNvYW"},"source":["##version 1: contours"]},{"cell_type":"code","metadata":{"id":"uKFvnj4zBT24"},"source":["import moviepy.editor as mp\n","from skimage import feature\n","from PIL import Image\n","\n","\n","vid_path = \"/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos\"\n","all_folders = glob.glob(\"%s/*_frames\" %vid_path)\n","for f in all_folders: #each video\n","  stim_name = f.split(\"/\")[-1].split(\"_\")[0]\n","  print(stim_name)\n","  all_frames = glob.glob(\"%s/*.jpg\" %f)\n","  \n","  for count in np.arange(1,len(all_frames)): #each frame\n","    f_name = f + \"/frame%d.jpg\" %count\n","    pil_image = PIL.Image.open(f_name).convert('RGB')\n","    img_original = numpy.array(pil_image)\n","    img_data = pil_to_tensor(pil_image)\n","    singleton_batch = {'img_data': img_data[None].cuda()}\n","    output_size = img_data.shape[1:]\n","    \n","    with torch.no_grad():\n","      scores = segmentation_module(singleton_batch, segSize=output_size)\n","\n","    # Get the predicted scores for each pixel\n","    _, pred = torch.max(scores, dim=1)\n","    pred = pred.cpu()[0].numpy()\n","    # visualize_result(img_original, pred)\n","\n","    allcounts = numpy.bincount(pred.flatten())\n","    predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]\n","    \n","    mask_count = 1\n","    for c in predicted_classes[:15]:\n","      mask_c = pred.copy()\n","      mask_c[mask_c != c] = 0\n","      mask_c[mask_c == c] = 255\n","      mask_c = mask_c.astype(np.uint8)\n","\n","      #get edges\n","      edges = feature.canny(mask_c, sigma=10).astype(\"uint8\")\n","      kernel = np.ones((10,10), np.uint8) \n","      edges = cv2.dilate(edges,kernel)\n","\n","      #get individual contours\n","      kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n","      opening = cv2.morphologyEx(mask_c, cv2.MORPH_OPEN, kernel, iterations=1)\n","      cnts = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","      cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n","      num_contors = len(cnts) \n","\n","      mask_size = len(np.where(mask_c==255)[0])\n","      # if it's not road, sky and mask size is large enough\n","      if c not in [1, 2, 6, 16] and mask_size >5000 and num_contors < 6:\n","        # print(c)\n","        # print(mask_size)\n","        if mask_count == 1:\n","          final_edges = edges\n","          final_edges = np.expand_dims(final_edges, axis=2)\n","          mask_count +=1\n","        else:\n","          final_edges = np.concatenate([final_edges, np.expand_dims(edges, axis=2)], axis = 2)\n","          mask_count +=1\n","\n","    if len(final_edges.shape)==3:\n","      final_edges = np.max(final_edges, axis = 2)\n","\n","    if os.path.isdir(\"%s/temp\" %(f))== False:\n","      os.mkdir(\"%s/temp\" %(f))\n","    print(\"frame %d\" %count)\n","    plt.imshow(final_edges, \"gray\")\n","    plt.axis(\"off\")\n","    filename = \"frame_%d_seg.jpg\" %count\n","    plt.savefig(\"%s/temp/%s\" %(f,filename), bbox_inches='tight', pad_inches=0)\n","    count +=1\n","  \n","  print(\"%s finished\" %stim_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zh2rcVIiNw7L"},"source":["## version2: mask + edges\n"]},{"cell_type":"code","metadata":{"id":"trU7F7B7WlzC"},"source":["from skimage import morphology\n","vid_path = \"/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos\"\n","all_folders = glob.glob(\"%s/*_frames\" %vid_path)\n","all_folders = all_folders[5:9]\n","\n","for f in all_folders: #each video\n","  stim_name = f.split(\"/\")[-1].split(\"_\")[0]\n","  print(stim_name)\n","  all_frames = glob.glob(\"%s/*.jpg\" %f)\n","  \n","  # W = 10\n","  # Mask_rep = {} \n","  # mask repository that is gonna been updated:\n","  # key is the class-name\n","  # value is 3d array : 2d mask x #frames(W)\n","\n","  for count in np.arange(1,len(all_frames)): #each frame\n","    f_name = f + \"/frame%d.jpg\" %count\n","    pil_image = PIL.Image.open(f_name).convert('RGB')\n","    img_original = numpy.array(pil_image)\n","    img_data = pil_to_tensor(pil_image)\n","    singleton_batch = {'img_data': img_data[None].cuda()}\n","    output_size = img_data.shape[1:]\n","\n","    with torch.no_grad():\n","      scores = segmentation_module(singleton_batch, segSize=output_size)\n","\n","    # Get the predicted scores for each pixel\n","    _, pred = torch.max(scores, dim=1)\n","    pred = pred.cpu()[0].numpy()\n","    # visualize_result(img_original, pred)\n","\n","    # filter out other classes\n","    classes = [1, 4, 11, 12, 17, 20, 51, 52, 80, 83, 128]\n","    pred_clean = pred.copy()\n","    pred_clean[~np.isin(pred_clean, classes)]= 0\n","\n","    # filter out small islands\n","    pred_clean2 = morphology.remove_small_objects(pred_clean.astype(bool), min_size=5000).astype(int)*255\n","\n","    # combine mask with correct class labels\n","    pred_clean3 = np.minimum(pred_clean, pred_clean2)\n","\n","    # update pred\n","    pred = pred_clean3\n","    allcounts = numpy.bincount(pred.flatten())\n","    predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]\n","\n","    mask_count = 1\n","    for c in predicted_classes[:11]:\n","      class_name = names[c+1]\n","      mask_c = pred.copy()\n","      mask_c[mask_c == c] = 255\n","      mask_c[mask_c != 255] = 0\n","      mask_c = mask_c.astype(np.uint8)\n","      mask_size = len(np.where(mask_c==255)[0])\n","      # if count < W: #if not observed 10 frames yet\n","      #   if class_name not in Mask_rep.keys():\n","      #     Mask_rep[class_name] = np.zeros([540, 960, W])\n","      #     Mask_rep[class_name][:,:,0] = mask_c\n","      #   else:\n","      #     Mask_rep[class_name][:,:,(count % W-1)] = mask_c\n","      # else:\n","      #   if class_name not in Mask_rep.keys():\n","      #     Mask_rep[class_name] = np.zeros([540, 960, W])\n","      #     Mask_rep[class_name][:,:,0] = mask_c\n","      if mask_size > 50000 and mask_size < 100000:\n","        # print(\"{} size {}\".format(class_name, mask_size))\n","        if mask_count == 1:\n","          final_mask = mask_c\n","          final_mask = np.expand_dims(final_mask, axis=2)\n","          mask_count +=1\n","        else:\n","          final_mask = np.concatenate([final_mask, np.expand_dims(mask_c, axis=2)], axis = 2)\n","          mask_count +=1\n","\n","    if len(final_mask.shape)==3:\n","      final_mask = np.max(final_mask, axis = 2)\n","\n","    if os.path.isdir(\"%s/temp_mask\" %(f))== False:\n","      os.mkdir(\"%s/temp_mask\" %(f))\n","\n","    print(\"frame %d\" %count)\n","    plt.imshow(final_mask, \"gray\")\n","    plt.axis(\"off\")\n","    filename = \"frame_%d_seg.jpg\" %count\n","    plt.savefig(\"%s/temp_mask/%s\" %(f,filename), bbox_inches='tight', pad_inches=0)\n","\n","    count +=1\n","\n","  print(\"%s finished\" %stim_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GbHMLG2tNe9O"},"source":["## version 3 only people, car, path edges"]},{"cell_type":"code","metadata":{"id":"UIcwwaL4Nd_v"},"source":["vid_path = \"/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos\"\n","all_folders = glob.glob(\"%s/*_frames\" %vid_path)\n","all_folders = all_folders[8:12]\n","for f in all_folders: #each video\n","  stim_name = f.split(\"/\")[-1].split(\"_\")[0]\n","  print(stim_name)\n","  all_frames = glob.glob(\"%s/*.jpg\" %f)\n","  \n","  # W = 10\n","  # Mask_rep = {} \n","  # mask repository that is gonna been updated:\n","  # key is the class-name\n","  # value is 3d array : 2d mask x #frames(W)\n","\n","  for count in np.arange(1,len(all_frames)): #each frame\n","    f_name = f + \"/frame%d.jpg\" %count\n","    pil_image = PIL.Image.open(f_name).convert('RGB')\n","    img_original = numpy.array(pil_image)\n","    img_data = pil_to_tensor(pil_image)\n","    singleton_batch = {'img_data': img_data[None].cuda()}\n","    output_size = img_data.shape[1:]\n","\n","    with torch.no_grad():\n","      scores = segmentation_module(singleton_batch, segSize=output_size)\n","\n","    # Get the predicted scores for each pixel\n","    _, pred = torch.max(scores, dim=1)\n","    pred = pred.cpu()[0].numpy()\n","    # visualize_result(img_original, pred)\n","\n","    # filter out other classes\n","    classes = [6, 11, 12, 13, 17, 20] #road, sidewalk, person, earth, plant, car\n","    pred_clean = pred.copy()\n","    pred_clean[~np.isin(pred_clean, classes)]= 0\n","\n","    # filter out small islands\n","    pred_clean2 = morphology.remove_small_objects(pred_clean.astype(bool), min_size=16000).astype(int)*255\n","\n","    # combine mask with correct class labels\n","    pred_clean3 = np.minimum(pred_clean, pred_clean2)\n","\n","    # get structure edges and get only long ones\n","    image = Image.fromarray(np.uint8(pred_clean3 * 255) , 'L')\n","    image_edge = image.filter(ImageFilter.FIND_EDGES) \n","    image_edge = np.array(image_edge)\n","    kernel = np.ones((10,10), np.uint8)\n","    image_edge = cv2.dilate(image_edge, kernel, iterations=1)\n","\n","    minLineLength = 200\n","    maxLineGap = 2\n","    lines = cv2.HoughLinesP(image_edge,1,np.pi/180,15,minLineLength=minLineLength,maxLineGap=maxLineGap)\n","    edges = np.zeros(pred_clean3.shape)\n","    try:\n","      for x in range(0, len(lines)):\n","        for x1,y1,x2,y2 in lines[x]:\n","          cv2.line(edges,(x1,y1),(x2,y2),color = (255, 255, 255))\n","      edges = cv2.dilate(edges, kernel, iterations=1)\n","    except (RuntimeError, TypeError, NameError):\n","      print(\"no lines\")\n","\n","    # update pred\n","    pred = pred_clean3\n","    allcounts = numpy.bincount(pred.flatten())\n","    predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]\n","    predicted_classes = [c for c in predicted_classes if c in [12, 20, 80, 83]] #mask only for person car, bus, truck\n","    mask_count = 1\n","    final_mask = np.zeros(pred.shape)\n","    for c in predicted_classes:\n","      class_name = names[c+1]\n","      mask_c = pred.copy()\n","      mask_c[mask_c == c] = 255\n","      mask_c[mask_c != 255] = 0\n","      mask_c = mask_c.astype(np.uint8)\n","      mask_size = len(np.where(mask_c==255)[0])\n","      \n","      if c==12 and mask_size>0: # find elongated direction for person\n","        # find outer contour\n","        ret, thresh = cv2.threshold(mask_c,0,255,0)\n","        cntrs = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","        cntrs = cntrs[0] if len(cntrs) == 2 else cntrs[1]\n","\n","        # get rotated rectangle from outer contour\n","        rotrect = cv2.minAreaRect(cntrs[0])\n","        box = cv2.boxPoints(rotrect)\n","        box = np.int0(box)\n","        # get angle from rotated rectangle\n","        angle = rotrect[-1]\n","        if np.abs(angle) < 60: #if the person elongated direction is horizontal, skip it (probably false alarm)\n","          continue\n","\n","      # print(\"{} size {}\".format(class_name, mask_size))\n","      if mask_count == 1:\n","        final_mask = mask_c\n","        final_mask = np.expand_dims(final_mask, axis=2)\n","        mask_count +=1\n","      else:\n","        final_mask = np.concatenate([final_mask, np.expand_dims(mask_c, axis=2)], axis = 2)\n","        mask_count +=1\n","\n","    if len(final_mask.shape)==3:\n","      final_mask = np.concatenate([final_mask, np.expand_dims(edges, axis=2)], axis = 2)\n","      final_mask = np.max(final_mask, axis = 2)\n","    else:\n","      final_mask = np.concatenate([np.expand_dims(final_mask,axis=2), np.expand_dims(edges, axis=2)], axis = 2)\n","      final_mask = np.max(final_mask, axis = 2)\n","\n","    if os.path.isdir(\"%s/temp_mask_edge\" %(f))== False:\n","      os.mkdir(\"%s/temp_mask_edge\" %(f))\n","\n","    print(\"frame %d\" %count)\n","    plt.imshow(final_mask, \"gray\")\n","    plt.axis(\"off\")\n","    filename = \"frame_%d_seg.jpg\" %count\n","    plt.savefig(\"%s/temp_mask_edge/%s\" %(f,filename), bbox_inches='tight', pad_inches=0)\n","\n","    count +=1\n","\n","  print(\"%s finished\" %stim_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIWJssKaKZQN"},"source":["## Make Videos"]},{"cell_type":"code","metadata":{"id":"DeUjkHr7WdWH","executionInfo":{"status":"ok","timestamp":1601610738497,"user_tz":420,"elapsed":611235,"user":{"displayName":"Nicole Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKeOj6Ama8Lt4IQOSJabK9SKp_WRexlsTL3CAf8Q=s64","userId":"17869521948231799953"}},"outputId":"5d1aa9c3-90b8-4a0a-de1d-6a408f2798d8","colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["import cv2\n","import numpy as np\n","import glob\n","fps = 20\n","size = (334, 188)\n","\n","vid_path = \"/content/gdrive/My Drive/291 Bionic Vision Project/Asa&Devi_Video/New Stimuli/Videos\"\n","all_folders = glob.glob(\"%s/*_frames\" %vid_path)\n","for f in all_folders: #each video\n","  stim_name = f.split(\"/\")[-1].split(\"_\")[0]\n","  vid_pathOut = vid_path + \"/%s_seg.avi\" %stim_name\n","  out = cv2.VideoWriter(vid_pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n","\n","  allframes = glob.glob('%s/temp/*.jpg' %f)\n","  for i in np.arange(1,len(allframes)):\n","    filename = '%s/temp/frame_%d_seg.jpg' %(f,i)\n","    img = cv2.imread(filename)\n","    out.write(img)\n","\n","  out.release()\n","  print(\"%s finished\" %stim_name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["stim1 finished\n","stim10 finished\n","stim11 finished\n","stim12 finished\n","stim13 finished\n","stim14 finished\n","stim15 finished\n","stim16 finished\n","stim2 finished\n","stim3 finished\n","stim4 finished\n","stim5 finished\n","stim6 finished\n","stim7 finished\n","stim8 finished\n","stim9 finished\n"],"name":"stdout"}]}]}